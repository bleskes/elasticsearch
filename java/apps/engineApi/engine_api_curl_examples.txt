Engine API Curl Examples
========================

This document assumes you have installed Prelert locally if not please update 
the host and port number in the Urls accordingly. 

0 Check Install
----------------
First check the installation by visiting http://localhost:8080/engine/v0.3/
in your browser or using the curl command:

    curl 'http://localhost:8080/engine/v0.3/'

If the REST API is running you will be greeted by a page containing the version
information.

1 Jobs
-------

After installing and starting the Engine the starting point is the Jobs endpoint

	curl 'http://localhost:8080/engine/v0.3/jobs'

As no jobs have been created this request will return an empty results document

{
  "hitCount" : 0,
  "skip" : 0,
  "take" : 100,
  "nextPage" : null,
  "previousPage" : null,
  "documents" : [ ]
}

Note hitCount = 0 and the documents array is empty. skip, take, nextPage and 
previousPage exist for paging results where the number of results is greater than 
the 'take' parameter. The default 'take' value is 100.

2. Create a Job
----------------

Here is an example of the CSV data where we wish analyse in a new job. Each 
record has as field called 'time' containing the time in ISO 8601 format
additionally there are 2 columns 'responsetime' and 'airline' we will apply anomaly
detection to. The raw csv data (in file farequote_ISO_8601.csv) looks like this

time,airline,responsetime,sourcetype
2013-01-28 00:00:00,AAL,132.2046,farequote
2013-01-28 00:00:00,JZA,990.4628,farequote
2013-01-28 00:00:00,JBU,877.5927,farequote
2013-01-28 00:00:00,KLM,1355.4812,farequote
2013-01-28 00:00:00,NKS,9991.3981,farequote

There are 2 parts to creating the job - configuring which fields are to be analysed
and by what function, secondly the API must be given a description of the format
the data is in.

2.1 Configure the fields for analysis
--------------------------------------
This analysisConfig says use a bucket span of 3600 seconds (1 hour) and analyse 
the field 'responsetime' by 'airline'.

    "analysisConfig" : {
        "bucketSpan":3600,
        "detectors" :[{"fieldName":"responsetime","byFieldName":"airline"}]     
    }



2.2 Configure the data input format
------------------------------------
The input data is in a delineated format each field delimited by ',' and each
record has a timestamp in the field 'time' and that timestamp is formatted as 
'yyyy-MM-dd HH:mm:ss'

    "dataDescription" : {
        "fieldDelimiter":",",
        "timeField":"time", 
        "timeFormat":"yyyy-MM-dd HH:mm:ss"
    }


Putting it all together we can create a new job by posting this config to the 
jobs endpoint

curl -v -X POST -H 'Content-Type: application/json' 'http://localhost:8080/engine/v0.3/jobs' -d '{
    "analysisConfig" : {
		"bucketSpan":3600,
		"detectors" :[{"fieldName":"responsetime","byFieldName":"airline"}]		
	},
    "dataDescription" : {
        "fieldDelimiter":",",
        "timeField":"time", 
        "timeFormat":"yyyy-MM-dd HH:mm:ss"
    }
 }'

This returns HTTP status code 201 and the id of the new job

{"id":"20140319115725-00009"}

The full Url to the new job is returned in the location header run curl with '-v' 
to see the HTTP headers.


Now get the details of the new job:

curl 'http://localhost:8080/engine/v0.3/jobs'

{
  "hitCount" : 1,
  "skip" : 0,
  "take" : 100,
  "nextPage" : null,
  "previousPage" : null,
  "documents" : [ {
    "timeout" : 600,
    "location" : "http://localhost:8080/engine/v0.3/jobs/20140319133023-00005",
    "id" : "20140319133023-00005",
    "analysisConfig" : {
      "bucketSpan" : 3600,
      "batchSpan" : null,
      "partitionField" : null,
      "detectors" : [ {
        "function" : null,
        "fieldName" : "responsetime",
        "byFieldName" : "airline",
        "overFieldName" : null,
        "useNull" : null
      } ],
      "period" : null
    },
    "analysisOptions" : null,
    "status" : "RUNNING",
    "dataDescription" : {
      "timeField" : "time",
      "timeFormat" : "yyyy-MM-dd HH:mm:ss",
      "fieldDelimiter" : ",",
      "format" : "DELINEATED"
    },
    "dataEndpoint" : "http://localhost:8080/engine/v0.3/data/20140319133023-00005",
    "resultsEndpoint" : "http://localhost:8080/engine/v0.3/results/20140319133023-00005",
    "logsEndpoint" : "http://localhost:8080/engine/v0.3/logs/20140319133023-00005",
    "finishedTime" : null,
    "lastDataTime" : null,
    "createTime" : "2014-03-19T13:30:23.152+0000",
    "processedRecordCount" : 0,
    "persistModel" : true
  } ]
}

The Jobs document is embedded inside a results paging object, note that 
hitCount = 1 and the documents array contains 1 Job document - the newly created 
job. nextPage and previousPage are still null because there aren't enough results 
to activate paging yet. 

The Job document contains 4 useful links
1) location - The Url to this job
2) dataEndpoint - This job's streaming data endpoint
3) resultsEndpoint - This job's results endpoint
4) logsEndpoint - Access the job's log files 

If you only want details of a particular job append the Job Id to the jobs 
endpoint (again don't use this url copy the 'location' url from the jobs doc).

    curl 'http://localhost:8080/engine/v0.3/jobs/20140319133023-00005'

When we request a single document, for example our new job, it is not returned 
inside a paging object with hitCount / paging Urls a different format is 
used containing a single document:

{
  "id" : "20140319115725-00009",
  "exists" : true,
  "type" : "job",
  "document" : {
    /* The requested document */
	...		
  }
}

If the doc cannot be found a 404 status code and error message is returned.


If we look at the results endpoint for the job we see there are no results yet.

    curl 'http://localhost:8080/engine/v0.3/results/20140319133023-00005'

returns the same empty results document we saw in section 1.
{
  "hitCount" : 0,
  "skip" : 0,
  "take" : 100,
  "nextPage" : null,
  "previousPage" : null,
  "documents" : [ ]
}


3. Upload data
---------------

In this example we are using the data in farequote_ISO_8601.csv.

    curl 'http://localhost:8080/engine/v0.3/data/20140319133023-00005' --data-binary @farequote_ISO_8601.csv

When using Curl remember to use --data-binary to preserve line endings in the 
text file

4. Results
-----------
You can get the results endpoint url from the job document 

    curl 'http://localhost:8080/engine/v0.3/results/20140319133023-00005'

Returns Paging object a list of bucket results
{
  "hitCount" : 118,
  "skip" : 0,
  "take" : 100,
  "nextPage" : "http://localhost:8080/engine/v0.3/results/20140319133023-00005?skip=100&take=100",
  "previousPage" : null,
  "documents" : [ {
    "recordCount" : 1,
    "timestamp" : "2013-01-28T00:00:00.000Z",
    "Id" : "1359331200",
    "anomalyScore" : 0.0
  }, {
    "recordCount" : 1,
    "timestamp" : "2013-01-28T01:00:00.000Z",
    "Id" : "1359334800",
    "anomalyScore" : 0.0
  }, {
  ...
  }
}

Get a single bucket:

    curl 'http://localhost:8080/engine/v0.3/results/20140319133023-00005/1359378000'

Get anomaly records in that bucket:

    curl 'http://localhost:8080/engine/v0.3/results/20140319133023-00005/1359378000/records'


Use the expand query option to get buckets with records

    curl 'http://localhost:8080/engine/v0.3/results/20140319133023-00005?expand=true'
    curl 'http://localhost:8080/engine/v0.3/results/20140319133023-00005/1359378000?expand=true'



5. Json Data Example
---------------------
The API also accepts Json data. For example here we have 3 Json objects each with
a 'timestamp' field in seconds (not milliseconds) since the Epoch which is the 
default format. Once again we want to analyse the field 'responsetime' by 
'ariline'

{"timestamp": 1350824400, "airline": "DJA", "responsetime": 622, "sourcetype": "flightcentre"}
{"timestamp": 1350824401, "airline": "JQA", "responsetime": 1742, "sourcetype": "flightcentre"}
{"timestamp": 1350824402, "airline": "GAL", "responsetime": 5339, "sourcetype": "flightcentre"}

The 'analysisConfig' is the same as before but 'dataDescription' now tells the API
that the data is in Json format and has a 'timestamp' containing the time in
seconds from the Epoch.

curl -v -X POST -H 'Content-Type: application/json' 'http://localhost:8080/engine/v0.3/jobs' -d '{
    "analysisConfig" : {
		"bucketSpan":3600,
		"detectors" :[{"fieldName":"responsetime","byFieldName":"airline"}]		
    },
    "dataDescription" : {
        "format":"json",
        "timeField":"timestamp"
    }
 }'

Returns
    {"id":"20140319133023-00006"}

and the data is uploaded using Curl
    curl 'http://localhost:8080/engine/v0.3/data/20140319133023-00006/' --data-binary @flightcentre.json


6. Gzipped Data Example
------------------------
This uses the same Farequote data but this time it is gzipped csv. The job 
configuration is similar to the other examples:

curl -v -X POST -H 'Content-Type: application/json' 'http://localhost:8080/engine/v0.3/jobs' -d '{
    "analysisConfig" : {
		"bucketSpan":3600,
		"detectors" :[{"fieldName":"responsetime","byFieldName":"airline"}]		
    },
    "dataDescription" : {
        "fieldDelimiter":","
    }
}'

To upload a gzipped compressed file the 'Content-Encoding' must bet set to 'gzip'

    curl -v -H'Content-Encoding:gzip' 'http://vm-centos-62-64-1:8080/engine/v0.3/data/20140319140313-00007/' --data-binary @flightcentre.csv.gz

